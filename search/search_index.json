{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS App Mesh Controller For K8s \u00b6 AWS App Mesh Controller For K8s is a controller to help manage App Mesh resources for a Kubernetes cluster. The controller watches custom resources for changes and reflects those changes into the App Mesh API . It is accompanied by the deployment of three custom resource definitions ( CRDs ): meshes, virtualnodes, and virtualservices. These map to App Mesh API objects which the controller manages for you. Documentation \u00b6 Checkout our Live Docs !","title":"Home"},{"location":"#aws-app-mesh-controller-for-k8s","text":"AWS App Mesh Controller For K8s is a controller to help manage App Mesh resources for a Kubernetes cluster. The controller watches custom resources for changes and reflects those changes into the App Mesh API . It is accompanied by the deployment of three custom resource definitions ( CRDs ): meshes, virtualnodes, and virtualservices. These map to App Mesh API objects which the controller manages for you.","title":"AWS App Mesh Controller For K8s"},{"location":"#documentation","text":"Checkout our Live Docs !","title":"Documentation"},{"location":"guide/installation/","text":"AppMesh Installation Guide \u00b6 Via Helm \u00b6 Follow instructions in appmesh-controller helm chart.","title":"Installation"},{"location":"guide/installation/#appmesh-installation-guide","text":"","title":"AppMesh Installation Guide"},{"location":"guide/installation/#via-helm","text":"Follow instructions in appmesh-controller helm chart.","title":"Via Helm"},{"location":"guide/monitoring/","text":"AppMesh Monitoring Guide \u00b6 AppMesh controller supports monitoring the control plane with Prometheus and Grafana. Install Prometheus \u00b6 Follow instructions in appmesh-prometheus helm chart. Install Grafana \u00b6 Follow instructions in appmesh-grafana helm chart.","title":"Monitoring"},{"location":"guide/monitoring/#appmesh-monitoring-guide","text":"AppMesh controller supports monitoring the control plane with Prometheus and Grafana.","title":"AppMesh Monitoring Guide"},{"location":"guide/monitoring/#install-prometheus","text":"Follow instructions in appmesh-prometheus helm chart.","title":"Install Prometheus"},{"location":"guide/monitoring/#install-grafana","text":"Follow instructions in appmesh-grafana helm chart.","title":"Install Grafana"},{"location":"guide/tracing/","text":"AppMesh Tracing Guide \u00b6 AppMesh controller supports integration with multiple tracing solutions for data plane. AWS X-Ray \u00b6 Enable X-Ray tracing for the App Mesh data plane helm upgrade -i appmesh-controller eks/appmesh-controller \\ --namespace appmesh-system \\ --set tracing.enabled = true \\ --set tracing.provider = x-ray The above configuration will inject the AWS X-Ray daemon sidecar in each pod scheduled to run on the mesh. Note : You should restart all pods running inside the mesh after enabling tracing. Datadog tracing \u00b6 Install the Datadog agent in the appmesh-system namespace Enable Datadog Tracing for the App Mesh data plane helm upgrade -i appmesh-controller eks/appmesh-controller \\ --namespace appmesh-system \\ --set tracing.enabled = true \\ --set tracing.provider = datadog \\ --set tracing.address = datadog.appmesh-system \\ --set tracing.port = 8126 Note : You should restart all pods running inside the mesh after enabling tracing. Jaeger tracing \u00b6 Follow instructions in appmesh-jaeger helm chart.","title":"Tracing"},{"location":"guide/tracing/#appmesh-tracing-guide","text":"AppMesh controller supports integration with multiple tracing solutions for data plane.","title":"AppMesh Tracing Guide"},{"location":"guide/tracing/#aws-x-ray","text":"Enable X-Ray tracing for the App Mesh data plane helm upgrade -i appmesh-controller eks/appmesh-controller \\ --namespace appmesh-system \\ --set tracing.enabled = true \\ --set tracing.provider = x-ray The above configuration will inject the AWS X-Ray daemon sidecar in each pod scheduled to run on the mesh. Note : You should restart all pods running inside the mesh after enabling tracing.","title":"AWS X-Ray"},{"location":"guide/tracing/#datadog-tracing","text":"Install the Datadog agent in the appmesh-system namespace Enable Datadog Tracing for the App Mesh data plane helm upgrade -i appmesh-controller eks/appmesh-controller \\ --namespace appmesh-system \\ --set tracing.enabled = true \\ --set tracing.provider = datadog \\ --set tracing.address = datadog.appmesh-system \\ --set tracing.port = 8126 Note : You should restart all pods running inside the mesh after enabling tracing.","title":"Datadog tracing"},{"location":"guide/tracing/#jaeger-tracing","text":"Follow instructions in appmesh-jaeger helm chart.","title":"Jaeger tracing"},{"location":"guide/troubleshooting/","text":"AppMesh Troubleshooting Guide \u00b6 Common Errors \u00b6 Exceeded pod count per VirtualNode limit \u00b6 AppMesh limits pod count per virtualNode. By default the limit is 10. Your can adjust this limit by adjust the \"Connected Envoy processes per virtual node\" service quota . Namespaces is not labeled correctly \u00b6 Namespaces must be labeled with two kind of labels: * appmesh.k8s.aws/sidecarInjectorWebhook: enabled is required on namespaces where pod should be injected with envoy sidecars. * customized labels to make mesh CustomResource selects the namespace via mesh.spec.namespaceSelector . (optional if you have a single Mesh selects all namespaces) Troubleshooting \u00b6 Tail the controller logs: export APPMESH_SYSTEM_NAMESPACE = appmesh-system kubectl logs -n \" ${ APPMESH_SYSTEM_NAMESPACE } \" -f --since 10s \\ $( kubectl get pods -n \" ${ APPMESH_SYSTEM_NAMESPACE } \" -o name | grep controller ) Tail envoy logs: export APPLICATION_NAMESPACE = <your namespace> export APPLICATION = <your pod or deployment> # i.e. deploy/my-app kubectl logs -n \" ${ APPLICATION_NAMESPACE } \" ${ APPLICATION_POD } \" envoy -f --since 10s View envoy configuration: export APPLICATION_NAMESPACE = <your namespace> export APPLICATION = <your pod> kubectl port-forward -n \" ${ APPLICATION_NAMESPACE } \" \\ $( kubectl get pod -n \" ${ APPLICATION_NAMESPACE } \" | grep \" ${ APPLICATION } \" | awk '{print $1}' ) \\ 9901 Then navigate to localhost:9901/ for the index or localhost:9901/config_dump for the envoy config.","title":"Troubleshooting"},{"location":"guide/troubleshooting/#appmesh-troubleshooting-guide","text":"","title":"AppMesh Troubleshooting Guide"},{"location":"guide/troubleshooting/#common-errors","text":"","title":"Common Errors"},{"location":"guide/troubleshooting/#exceeded-pod-count-per-virtualnode-limit","text":"AppMesh limits pod count per virtualNode. By default the limit is 10. Your can adjust this limit by adjust the \"Connected Envoy processes per virtual node\" service quota .","title":"Exceeded pod count per VirtualNode limit"},{"location":"guide/troubleshooting/#namespaces-is-not-labeled-correctly","text":"Namespaces must be labeled with two kind of labels: * appmesh.k8s.aws/sidecarInjectorWebhook: enabled is required on namespaces where pod should be injected with envoy sidecars. * customized labels to make mesh CustomResource selects the namespace via mesh.spec.namespaceSelector . (optional if you have a single Mesh selects all namespaces)","title":"Namespaces is not labeled correctly"},{"location":"guide/troubleshooting/#troubleshooting","text":"Tail the controller logs: export APPMESH_SYSTEM_NAMESPACE = appmesh-system kubectl logs -n \" ${ APPMESH_SYSTEM_NAMESPACE } \" -f --since 10s \\ $( kubectl get pods -n \" ${ APPMESH_SYSTEM_NAMESPACE } \" -o name | grep controller ) Tail envoy logs: export APPLICATION_NAMESPACE = <your namespace> export APPLICATION = <your pod or deployment> # i.e. deploy/my-app kubectl logs -n \" ${ APPLICATION_NAMESPACE } \" ${ APPLICATION_POD } \" envoy -f --since 10s View envoy configuration: export APPLICATION_NAMESPACE = <your namespace> export APPLICATION = <your pod> kubectl port-forward -n \" ${ APPLICATION_NAMESPACE } \" \\ $( kubectl get pod -n \" ${ APPLICATION_NAMESPACE } \" | grep \" ${ APPLICATION } \" | awk '{print $1}' ) \\ 9901 Then navigate to localhost:9901/ for the index or localhost:9901/config_dump for the envoy config.","title":"Troubleshooting"},{"location":"reference/api_design/","text":"AppMesh-K8s integration v1beta2 CRD API design \u00b6 Introduction \u00b6 AppMesh is a service mesh product offered by Amazon. EKS team owns a controller that provides integration between AppMesh and Kubernetes using a custom resource (CRD) API. This document proposes a new version(v1beta2) of CRD API to address issues in current API design(v1beta1). What is the problem \u00b6 There are several issues in the old CRD API model. Details in AppMesh-K8s integration API issues . In summary: The name of Kubernetes resources is tightly coupled with AppMesh names, which is inflexible and confusing. The references between Kubernetes resources is ambiguous and relies on string hacks. The security model for Kubernetes resources is not properly defined. The VirtualNode CRD is implicitly coupled with Deployment resource. The VirtualService cannot directly use VirtualNode as provider Why is it changing \u00b6 We plan to announce k8s AppMesh controller as GA, and we should provide a stable and clean API for better customer experience. Out of scope \u00b6 The detailed implementation of new API is not discussed in this doc. Roles: \u00b6 This document considers two user role: Cluster Administrator: The cluster administrator (ops) is responsible for administration of entire clusters. They manage the lifecycle of AppMesh mesh and provide them to application developers. Application Developer : The application developer (dev) is responsible for defining their application configuration and service composition. Use cases: \u00b6 As a Cluster Administrator for multi-tenant EKS cluster, I should be able to allocate a service mesh and control which users can create resources in this mesh. As an Application Developer, I should be able to define the service routing logic and backend configuration for my application without Cluster Administrator's intervention. Tenets: \u00b6 The API should follow Kubernetes API conventions The API should be unambiguous, there should be only one obvious way for given task The API should be explicit instead of rely on implicit logic The API should be simple for 99% use case The API should be open for future extensions Design \u00b6 API overview \u00b6 We\u2019ll use four CRD to represent AppMesh objects: Mesh: Represents an AppMesh mesh object. VirtualNode: Represents an AppMesh virtualNode object. VirtualService: Represents an AppMesh virtualService object, which can use either VirtualRouter or VirtualNode as provider. VirtualRouter: Represents an AppMesh virtualRouter object and embeds virtualRoute. Detailed changes are discussed below: Decouple k8s resource name from AppMesh resource name \u00b6 Resource names for k8s resources should be decoupled from appMesh resource names. Users should be able to freely denote the resource name for k8s resources independently from appMesh resource names. Sensible default for appMesh resource names can be derived from k8s resource name . Mesh resource have a field \u201c awsName \u201d to denote the AppMesh name for mesh. It defaults to be same as the name of mesh k8s resource. Note: we may consider use other field name like \u201cmeshName\u201d, \u201cawsMeshName\u201d. we choose \u201cawsName\u201d over simply \u201cname\u201d to avoid confusing. This enables use cases like use same resource name(e.g. \u201c global \u201d) for the k8s Mesh resource in different EKS clusters, while maps to different appMesh name. apiVersion : appmesh.k8s.aws/v1beta2 kind : Mesh metadata : name : global spec : awsName : my-cluster-mesh VirtualService resource have a field \u201c awsName \u201d to denote the AppMesh name for VirtualService. It defaults to be \"$name.$namespace\" of VirtualService k8s resource. Note: the controller is responsible for detecting conflicting awsName with a mesh and report errors . apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualService metadata : namespace : my-app-ns name : svc-a spec : awsName : svc-a.my-app-ns Alternative designs considered: Enforce bind appMesh virtualServiceName to be VirtualService resource\u2019s name. ( this is existing design used in our controller ) apiVersion : appmesh.k8s.aws/v1beta1 kind : VirtualService metadata : namespace : my-app-ns name : my-app-node.my-app-ns.svc.cluster.local spec : ... Pros: * the k8s namespace serves as a scope as a weak guarantee for uniqueness of hostNames. (it\u2019s weak guarantee since there can still be possible hostName conflicts for resources in multiple namespaces) Cons: * harder to extend to multiple hostName support * name of k8s resource tied to appMesh name have an array field \u201c hosts \u201d to denote the AppMesh names for VirtualService For each host, we\u2019ll transparently create an appMesh VirtualService object . It defaults to have single entry of the VirtualService resource\u2019s name. apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualService metadata : namespace : my-app-ns name : my-app-svc spec : hosts : - my-app-svc.my-app-ns - my-app-svc.my-app-ns.svc.cluster.local Pros: * user friendly if multiple hostname for a service is desired. (under k8s, a pod can use multiple name to contact a service with the help of DNS search list). Cons: * not align with AppMesh API. (we decided not to do this unless appMesh itself supports such API). VirtualNode resource can have a field \u201c awsName \u201d to denote the AppMesh name for VirtualNode. It defaults to be \u201c ${name}_${namespace} \u201d of the VirtualNode resource. Note: we may consider use other field name like \u201cvirtualNodeName\u201d, \u201cawsVirtualNodeName\u201d. we choose \u201cawsName\u201d over simply \u201cname\u201d to avoid confusing. Note: currently controller use \"-\" as separator when build name, but we plan to change it to \"_\" to avoid conflicts with k8s's naming restrictions . This enables use cases like have different app mesh VirtualNode under different context. e.g. for a mesh designated for a single EKS cluster, the default can be \u201c ${name}_${namespace} \u201d. And for a mesh designated to be shared by multiple EKS clusters, the default can be \u201c ${name} ${namespace} ${clusterName} \u201d. The naming pattern can be controlled by immutable attributes on mesh resource in the future(e.g. spec.multiCluster ) apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualNode metadata : namespace : my-app-ns name : node-v1 spec : awsName : node-v1_my-app-ns Use typed references to reference resources within cluster \u00b6 we can use typed reference for relationship between resources within cluster. use virtualNodeRef for reference from VirtualRouter to VirtualNode apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualRouter metadata : namespace : my-app-ns name : svc-a spec : awsName : svc-a_my-app-ns listeners : - portMapping : port : 9080 protocol : http routes : - name : route-to-auth http : match : prefix : /auth action : weightedTargets : - virtualNodeRef : namespace : my-app-ns # can be omitted if in same namespace as VirtualRouter name : node-v1 weight : 1 Alternative designs considered: use mangled name of k8s virtualNode resource. ( this is existing design used in our controller ) We can use a mangled name of virtualNode resource. like \u201c ${nodeName} \u201d when referencing a virtualNode within current namespace and \u201c ${nodeName}.${namespace} \u201d when referencing a virtualNode in another namespace. weightedTargets : - virtualNodeName : colorteller.appmesh-demo weight : 4 - virtualNodeName : colorteller-blue weight : 3 - virtualNodeName : colorteller-black.appmesh-demo weight : 3 Cons: * VirtualNodeName here is ambiguous. It\u2019s referencing the name of k8s VirtualNode instead of appMesh VirtualNode. * It string hacks by whether there is a \u201c.\u201d inside and use it as name/namespace separator. while \u201c.\u2018 is valid to be part of name identifier in k8s. * It cannot be extended to support reference external virtualNode in mesh (like a ECS node) use the real name of appMesh virtualNode. We can use the real name of appMesh virtualNode instead of mangled name of k8s object. The above example will be: weightedTargets : - virtualNodeName : colorteller_appmesh-demo weight : 4 - virtualNodeName : colorteller-blue weight : 3 - virtualNodeName : colorteller-black_appmesh-demo weight : 3 Pros: * unambiguous supported both virtualNode in k8s and external like ECS. * simple implementation Cons: * users need to be aware of the name pattern of appMesh objects when want to reference a virtualNode created within cluster. however, it\u2019s subject to change, e.g. we may include a cluster identifier in appMesh name as well if we plan to support multiple cluster. * not k8s native for reference between objects [ optional, not planned ] virtualNodeRef can be extended to support resources outside of k8s cluster. e.g. a ECS virtual node. Note: we using field like \u201cawsName\u201d instead of \u201cvirtualNodeName\u201d to avoid confusing, since \u201cvirtualNodeName\u201d means \u201cname for a k8s virtualNode resource\u201d under k8s\u2019s convention. ```yaml apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualRouter metadata: namespace: my-app-ns name: svc-a spec: awsName: svc-a_my-app-ns listeners: - portMapping: port: 9080 protocol: http routes: - name: route-to-auth http: match: prefix: /auth action: weightedTargets: - virtualNodeRef: awsName: my-ecs-node weight: 1 1. use virtualServiceRef for references from VirtualNode to VirtualService apiVersion : appmesh.k8s.aws/v1beta1 kind : VirtualNode metadata : namespace : my-app-ns name : my-app-node spec : ...other fileds... backends : - virtualService : virtualServiceRef : namespace : my-app-ns # can be omitted if in same namespace as VirtualNode name : my-app-svc Alternative designs considered: use the real name of appMesh virtualService. ( this is existing design used in our controller ) We can use the real name of appMesh virtualService when referencing it in nodes. Pros: * unambiguous supported both virtualService in k8s and external like ECS. * simple implementation Cons: * not k8s native for reference between objects * not consistent with how we reference nodes from services. [ optional, not planned ] virtualServiceRef can be extended to support resources outside of k8s cluster. e.g. a ECS virtual service. Note: we are using field like \u201cawsName\u201d instead of \u201cvirtualServiceName\u201d to avoid confusing, since \u201cvirtualServiceName\u201d means \u201cname for a k8s virtualService resource\u201d under k8s\u2019s convention. apiVersion : appmesh.k8s.aws/v1beta1 kind : VirtualNode metadata : namespace : my-app-ns name : my-app-node spec : ...other fields... backends : - virtualService : virtualServiceRef : awsName : \"my-ecs-service\" Decouple VirtualRouter from VirtualService \u00b6 In AppMesh a VirtualService can use either VirtualRouter or VirtualNode as provider. VirtualRouter is only need when L7 routing functionality is needed. We\u2019ll create a new CRD named VirtualRouter , which represents AppMesh VirtualRouter object and embeds multiple AppMesh VirtualRoute objects. apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualRouter metadata : namespace : my-app-ns name : svc-a spec : awsName : svc-a_my-app-ns listeners : - portMapping : port : 9080 protocol : http routes : - name : route-to-auth http : match : prefix : /auth action : weightedTargets : - virtualNodeRef : namespace : my-app-ns name : node-v1 weight : 1 Note: The additional hierarchy of \"virtualRouter\" and \"virtualNode\" in addition to \"virtualRouterRef\" and \"virtualNodeRef\" is to align with AppMesh API(so that additional attributes specific to provider can be added easily). VirtualService can reference virtualRouter as provider: apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualService metadata : namespace : my-app-ns name : svc-a spec : hosts : - svc-b.my-app-ns.svc.cluster.local provider : virtualRouter : virtualRouterRef : namespace : my-app-ns name : svc-a-router VirtualService can reference virtualNode as provider: apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualService metadata : namespace : my-app-ns name : svc-b spec : hosts : - svc-a.my-app-ns.svc.cluster.local provider : virtualNode : virtualNodeRef : namespace : my-app-ns name : node-v2 Use selector on Mesh to denote mesh membership for resources within namespaces \u00b6 Meshes should be set up by cluster administrator, they can use a selector to designate the mesh membership for resources in different k8s namespaces. Note: The label selectors of two mesh must not overlap. Controller should error if detected conflicting meshes for given resource. Initially, we can assume all resources in a single namespace will always belong to same mesh, so we only need namespaceSelector on Mesh resource. namespaceSelector follows standard label selector semantics, if presents not empty, it selects all namespaces. This also enables use cases like share a AppMesh application configuration since the application specific configuration(virtualNode / virtualService) no longer rely on specific mesh presents. apiVersion : appmesh.k8s.aws/v1beta2 kind : Mesh metadata : name : global spec : namespaceSelector : matchLabels : mesh : my-mesh --- apiVersion : v1 kind : Namespace metadata : name : my-app-ns labels : mesh : my-mesh [ optional, not planned ] If we need to support resources in a single namespace to belong to different meshes, we can extend above to have a resourceSelector to match labels on resources like VirtualService and VirtualNode in addition to the namespaceSelector. apiVersion : appmesh.k8s.aws/v1beta2 kind : Mesh metadata : name : my-mesh spec : namespaceSelector : matchLabels : mesh : my-mesh resourceSelector : matchLabels : mesh : my-mesh Alternative designs considered: 1. Use meshName on resources to denote mesh membership. ( this is existing design used in our controller ) Currently we are using a meshName on resources to denote mesh membership . ``` yaml apiVersion : appmesh . k8s . aws / v1beta1 kind : VirtualService metadata : namespace : my - app - ns name : my - app - svc spec : meshName : my - mesh ... other fields ... ``` Pros : * enforced that a resource can only belong to a single mesh . * simple implementation for lookup mesh based on resource . since the meshName is included on resources directly . Cons : * poor security model . a less privileged user limited within namespace can create resources into mesh freely . * requires user to set meshName on every resource even within a namespace . Use annotation on namespaces to denote mesh membership. We can use annotations on namespaces to denote mesh membership. apiVersion : v1 kind : Namespace metadata : name : my-app-ns annotations : appmesh.k8s.aws/mesh-name : my-mesh Pros: * enforced that a resource in a namespace can only belong to a single mesh * simple implementation for lookup mesh based on resource. since the meshName is included on resources\u2019s namespace directly. Cons: * not Kubernetes native. The Kubernetes native way is to use label selector to denote membership. * not possible for extensions like for a single namespace into multiple meshes in the future. Use an array of selectors instead of a single one on mesh resource. Selectors among selectors array are \u201c OR \u201d relationship. And within a selector it\u2019s AND relationship(e.g. resources must match both namespaceSelector & resourceSelector to be member of the mesh) apiVersion : appmesh.k8s.aws/v1beta2 kind : Mesh metadata : name : my-mesh spec : selectors : - namespaceSelector : matchLabels : mesh : my-mesh - namespaceSelector : matchLabels : my-mesh : \"true\" resourceSelector : matchLabels : my-mesh : \"true\" Pros: * let user freely define selectors for different use cases. Cons: * over complicated for simple use cases. users should be properly organize their labels to achieve similar effect even under the single selector proposal. Use selector on VirtualNode to denote node membership \u00b6 We use a podSelector to match pods that should be member of virtualNode. Note, we don\u2019t use a namespace selector here since we want to enforce pods are in same namespace of virtualNode resource. The label selectors of two virtualNode within namespace must not overlap. Controller should error if detected conflicting virtualNode for given pod. The injector will be changed to work using following logic: 1. for each pod created, it match against all virtualNode\u2019s selector within namespace. 2. If it finds a single match, injects according to virtualNode\u2019s mesh and name. 3. If it finds multiple match, it reports error. 4. The global \" APPMESH_NAME \" environment variable on injector deployment and annotation \" appmesh.k8s.aws/mesh \" and \" appmesh.k8s.aws/virtualNode \" annotation on pod will no longer be supported. apiVersion : appmesh.k8s.aws/v1beta1 kind : VirtualNode metadata : namespace : my-app-ns name : my-app-node spec : ...other fields... podSelector : matchLabels : app : my-app-node --- apiVersion : apps/v1 kind : Deployment metadata : namespace : my-app-ns name : my-app-node-deployment spec : selector : matchLabels : app : my-app-node replicas : 5 template : metadata : labels : app : my-app-node spec : containers : - name : app image : awesomeimage:v1.0.0 ports : - containerPort : 80 Alternative designs considered: The injector use an environment variable for default mesh name and allow override per pod with annotation appmesh.k8s.aws/mesh . Also, the virtualNodeName is derived from pod\u2019s replicaSet\u2019s deployment name and allows override per pod with annotation appmesh.k8s.aws/virtualNode ( this is existing design used in our controller ) Pros: * easy to implement since all information is on pod itself. Cons: * The default behavior to resolve mesh relies on the fact that there is only one mesh in cluster, which conflicts with our multiple mesh per cluster support. * The default behavior to resolve virtualNode is not k8s native. it implies pods are managed by replicaSets then by deployments, which is tricky and not always true. * the annotations are intrusive to user\u2019s application.","title":"APIDesign"},{"location":"reference/api_design/#appmesh-k8s-integration-v1beta2-crd-api-design","text":"","title":"AppMesh-K8s integration v1beta2 CRD API design"},{"location":"reference/api_design/#introduction","text":"AppMesh is a service mesh product offered by Amazon. EKS team owns a controller that provides integration between AppMesh and Kubernetes using a custom resource (CRD) API. This document proposes a new version(v1beta2) of CRD API to address issues in current API design(v1beta1).","title":"Introduction"},{"location":"reference/api_design/#what-is-the-problem","text":"There are several issues in the old CRD API model. Details in AppMesh-K8s integration API issues . In summary: The name of Kubernetes resources is tightly coupled with AppMesh names, which is inflexible and confusing. The references between Kubernetes resources is ambiguous and relies on string hacks. The security model for Kubernetes resources is not properly defined. The VirtualNode CRD is implicitly coupled with Deployment resource. The VirtualService cannot directly use VirtualNode as provider","title":"What is the problem"},{"location":"reference/api_design/#why-is-it-changing","text":"We plan to announce k8s AppMesh controller as GA, and we should provide a stable and clean API for better customer experience.","title":"Why is it changing"},{"location":"reference/api_design/#out-of-scope","text":"The detailed implementation of new API is not discussed in this doc.","title":"Out of scope"},{"location":"reference/api_design/#roles","text":"This document considers two user role: Cluster Administrator: The cluster administrator (ops) is responsible for administration of entire clusters. They manage the lifecycle of AppMesh mesh and provide them to application developers. Application Developer : The application developer (dev) is responsible for defining their application configuration and service composition.","title":"Roles:"},{"location":"reference/api_design/#use-cases","text":"As a Cluster Administrator for multi-tenant EKS cluster, I should be able to allocate a service mesh and control which users can create resources in this mesh. As an Application Developer, I should be able to define the service routing logic and backend configuration for my application without Cluster Administrator's intervention.","title":"Use cases:"},{"location":"reference/api_design/#tenets","text":"The API should follow Kubernetes API conventions The API should be unambiguous, there should be only one obvious way for given task The API should be explicit instead of rely on implicit logic The API should be simple for 99% use case The API should be open for future extensions","title":"Tenets:"},{"location":"reference/api_design/#design","text":"","title":"Design"},{"location":"reference/api_design/#api-overview","text":"We\u2019ll use four CRD to represent AppMesh objects: Mesh: Represents an AppMesh mesh object. VirtualNode: Represents an AppMesh virtualNode object. VirtualService: Represents an AppMesh virtualService object, which can use either VirtualRouter or VirtualNode as provider. VirtualRouter: Represents an AppMesh virtualRouter object and embeds virtualRoute. Detailed changes are discussed below:","title":"API overview"},{"location":"reference/api_design/#decouple-k8s-resource-name-from-appmesh-resource-name","text":"Resource names for k8s resources should be decoupled from appMesh resource names. Users should be able to freely denote the resource name for k8s resources independently from appMesh resource names. Sensible default for appMesh resource names can be derived from k8s resource name . Mesh resource have a field \u201c awsName \u201d to denote the AppMesh name for mesh. It defaults to be same as the name of mesh k8s resource. Note: we may consider use other field name like \u201cmeshName\u201d, \u201cawsMeshName\u201d. we choose \u201cawsName\u201d over simply \u201cname\u201d to avoid confusing. This enables use cases like use same resource name(e.g. \u201c global \u201d) for the k8s Mesh resource in different EKS clusters, while maps to different appMesh name. apiVersion : appmesh.k8s.aws/v1beta2 kind : Mesh metadata : name : global spec : awsName : my-cluster-mesh VirtualService resource have a field \u201c awsName \u201d to denote the AppMesh name for VirtualService. It defaults to be \"$name.$namespace\" of VirtualService k8s resource. Note: the controller is responsible for detecting conflicting awsName with a mesh and report errors . apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualService metadata : namespace : my-app-ns name : svc-a spec : awsName : svc-a.my-app-ns Alternative designs considered: Enforce bind appMesh virtualServiceName to be VirtualService resource\u2019s name. ( this is existing design used in our controller ) apiVersion : appmesh.k8s.aws/v1beta1 kind : VirtualService metadata : namespace : my-app-ns name : my-app-node.my-app-ns.svc.cluster.local spec : ... Pros: * the k8s namespace serves as a scope as a weak guarantee for uniqueness of hostNames. (it\u2019s weak guarantee since there can still be possible hostName conflicts for resources in multiple namespaces) Cons: * harder to extend to multiple hostName support * name of k8s resource tied to appMesh name have an array field \u201c hosts \u201d to denote the AppMesh names for VirtualService For each host, we\u2019ll transparently create an appMesh VirtualService object . It defaults to have single entry of the VirtualService resource\u2019s name. apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualService metadata : namespace : my-app-ns name : my-app-svc spec : hosts : - my-app-svc.my-app-ns - my-app-svc.my-app-ns.svc.cluster.local Pros: * user friendly if multiple hostname for a service is desired. (under k8s, a pod can use multiple name to contact a service with the help of DNS search list). Cons: * not align with AppMesh API. (we decided not to do this unless appMesh itself supports such API). VirtualNode resource can have a field \u201c awsName \u201d to denote the AppMesh name for VirtualNode. It defaults to be \u201c ${name}_${namespace} \u201d of the VirtualNode resource. Note: we may consider use other field name like \u201cvirtualNodeName\u201d, \u201cawsVirtualNodeName\u201d. we choose \u201cawsName\u201d over simply \u201cname\u201d to avoid confusing. Note: currently controller use \"-\" as separator when build name, but we plan to change it to \"_\" to avoid conflicts with k8s's naming restrictions . This enables use cases like have different app mesh VirtualNode under different context. e.g. for a mesh designated for a single EKS cluster, the default can be \u201c ${name}_${namespace} \u201d. And for a mesh designated to be shared by multiple EKS clusters, the default can be \u201c ${name} ${namespace} ${clusterName} \u201d. The naming pattern can be controlled by immutable attributes on mesh resource in the future(e.g. spec.multiCluster ) apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualNode metadata : namespace : my-app-ns name : node-v1 spec : awsName : node-v1_my-app-ns","title":"Decouple k8s resource name from AppMesh resource name"},{"location":"reference/api_design/#use-typed-references-to-reference-resources-within-cluster","text":"we can use typed reference for relationship between resources within cluster. use virtualNodeRef for reference from VirtualRouter to VirtualNode apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualRouter metadata : namespace : my-app-ns name : svc-a spec : awsName : svc-a_my-app-ns listeners : - portMapping : port : 9080 protocol : http routes : - name : route-to-auth http : match : prefix : /auth action : weightedTargets : - virtualNodeRef : namespace : my-app-ns # can be omitted if in same namespace as VirtualRouter name : node-v1 weight : 1 Alternative designs considered: use mangled name of k8s virtualNode resource. ( this is existing design used in our controller ) We can use a mangled name of virtualNode resource. like \u201c ${nodeName} \u201d when referencing a virtualNode within current namespace and \u201c ${nodeName}.${namespace} \u201d when referencing a virtualNode in another namespace. weightedTargets : - virtualNodeName : colorteller.appmesh-demo weight : 4 - virtualNodeName : colorteller-blue weight : 3 - virtualNodeName : colorteller-black.appmesh-demo weight : 3 Cons: * VirtualNodeName here is ambiguous. It\u2019s referencing the name of k8s VirtualNode instead of appMesh VirtualNode. * It string hacks by whether there is a \u201c.\u201d inside and use it as name/namespace separator. while \u201c.\u2018 is valid to be part of name identifier in k8s. * It cannot be extended to support reference external virtualNode in mesh (like a ECS node) use the real name of appMesh virtualNode. We can use the real name of appMesh virtualNode instead of mangled name of k8s object. The above example will be: weightedTargets : - virtualNodeName : colorteller_appmesh-demo weight : 4 - virtualNodeName : colorteller-blue weight : 3 - virtualNodeName : colorteller-black_appmesh-demo weight : 3 Pros: * unambiguous supported both virtualNode in k8s and external like ECS. * simple implementation Cons: * users need to be aware of the name pattern of appMesh objects when want to reference a virtualNode created within cluster. however, it\u2019s subject to change, e.g. we may include a cluster identifier in appMesh name as well if we plan to support multiple cluster. * not k8s native for reference between objects [ optional, not planned ] virtualNodeRef can be extended to support resources outside of k8s cluster. e.g. a ECS virtual node. Note: we using field like \u201cawsName\u201d instead of \u201cvirtualNodeName\u201d to avoid confusing, since \u201cvirtualNodeName\u201d means \u201cname for a k8s virtualNode resource\u201d under k8s\u2019s convention. ```yaml apiVersion: appmesh.k8s.aws/v1beta2 kind: VirtualRouter metadata: namespace: my-app-ns name: svc-a spec: awsName: svc-a_my-app-ns listeners: - portMapping: port: 9080 protocol: http routes: - name: route-to-auth http: match: prefix: /auth action: weightedTargets: - virtualNodeRef: awsName: my-ecs-node weight: 1 1. use virtualServiceRef for references from VirtualNode to VirtualService apiVersion : appmesh.k8s.aws/v1beta1 kind : VirtualNode metadata : namespace : my-app-ns name : my-app-node spec : ...other fileds... backends : - virtualService : virtualServiceRef : namespace : my-app-ns # can be omitted if in same namespace as VirtualNode name : my-app-svc Alternative designs considered: use the real name of appMesh virtualService. ( this is existing design used in our controller ) We can use the real name of appMesh virtualService when referencing it in nodes. Pros: * unambiguous supported both virtualService in k8s and external like ECS. * simple implementation Cons: * not k8s native for reference between objects * not consistent with how we reference nodes from services. [ optional, not planned ] virtualServiceRef can be extended to support resources outside of k8s cluster. e.g. a ECS virtual service. Note: we are using field like \u201cawsName\u201d instead of \u201cvirtualServiceName\u201d to avoid confusing, since \u201cvirtualServiceName\u201d means \u201cname for a k8s virtualService resource\u201d under k8s\u2019s convention. apiVersion : appmesh.k8s.aws/v1beta1 kind : VirtualNode metadata : namespace : my-app-ns name : my-app-node spec : ...other fields... backends : - virtualService : virtualServiceRef : awsName : \"my-ecs-service\"","title":"Use typed references to reference resources within cluster"},{"location":"reference/api_design/#decouple-virtualrouter-from-virtualservice","text":"In AppMesh a VirtualService can use either VirtualRouter or VirtualNode as provider. VirtualRouter is only need when L7 routing functionality is needed. We\u2019ll create a new CRD named VirtualRouter , which represents AppMesh VirtualRouter object and embeds multiple AppMesh VirtualRoute objects. apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualRouter metadata : namespace : my-app-ns name : svc-a spec : awsName : svc-a_my-app-ns listeners : - portMapping : port : 9080 protocol : http routes : - name : route-to-auth http : match : prefix : /auth action : weightedTargets : - virtualNodeRef : namespace : my-app-ns name : node-v1 weight : 1 Note: The additional hierarchy of \"virtualRouter\" and \"virtualNode\" in addition to \"virtualRouterRef\" and \"virtualNodeRef\" is to align with AppMesh API(so that additional attributes specific to provider can be added easily). VirtualService can reference virtualRouter as provider: apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualService metadata : namespace : my-app-ns name : svc-a spec : hosts : - svc-b.my-app-ns.svc.cluster.local provider : virtualRouter : virtualRouterRef : namespace : my-app-ns name : svc-a-router VirtualService can reference virtualNode as provider: apiVersion : appmesh.k8s.aws/v1beta2 kind : VirtualService metadata : namespace : my-app-ns name : svc-b spec : hosts : - svc-a.my-app-ns.svc.cluster.local provider : virtualNode : virtualNodeRef : namespace : my-app-ns name : node-v2","title":"Decouple VirtualRouter from VirtualService"},{"location":"reference/api_design/#use-selector-on-mesh-to-denote-mesh-membership-for-resources-within-namespaces","text":"Meshes should be set up by cluster administrator, they can use a selector to designate the mesh membership for resources in different k8s namespaces. Note: The label selectors of two mesh must not overlap. Controller should error if detected conflicting meshes for given resource. Initially, we can assume all resources in a single namespace will always belong to same mesh, so we only need namespaceSelector on Mesh resource. namespaceSelector follows standard label selector semantics, if presents not empty, it selects all namespaces. This also enables use cases like share a AppMesh application configuration since the application specific configuration(virtualNode / virtualService) no longer rely on specific mesh presents. apiVersion : appmesh.k8s.aws/v1beta2 kind : Mesh metadata : name : global spec : namespaceSelector : matchLabels : mesh : my-mesh --- apiVersion : v1 kind : Namespace metadata : name : my-app-ns labels : mesh : my-mesh [ optional, not planned ] If we need to support resources in a single namespace to belong to different meshes, we can extend above to have a resourceSelector to match labels on resources like VirtualService and VirtualNode in addition to the namespaceSelector. apiVersion : appmesh.k8s.aws/v1beta2 kind : Mesh metadata : name : my-mesh spec : namespaceSelector : matchLabels : mesh : my-mesh resourceSelector : matchLabels : mesh : my-mesh Alternative designs considered: 1. Use meshName on resources to denote mesh membership. ( this is existing design used in our controller ) Currently we are using a meshName on resources to denote mesh membership . ``` yaml apiVersion : appmesh . k8s . aws / v1beta1 kind : VirtualService metadata : namespace : my - app - ns name : my - app - svc spec : meshName : my - mesh ... other fields ... ``` Pros : * enforced that a resource can only belong to a single mesh . * simple implementation for lookup mesh based on resource . since the meshName is included on resources directly . Cons : * poor security model . a less privileged user limited within namespace can create resources into mesh freely . * requires user to set meshName on every resource even within a namespace . Use annotation on namespaces to denote mesh membership. We can use annotations on namespaces to denote mesh membership. apiVersion : v1 kind : Namespace metadata : name : my-app-ns annotations : appmesh.k8s.aws/mesh-name : my-mesh Pros: * enforced that a resource in a namespace can only belong to a single mesh * simple implementation for lookup mesh based on resource. since the meshName is included on resources\u2019s namespace directly. Cons: * not Kubernetes native. The Kubernetes native way is to use label selector to denote membership. * not possible for extensions like for a single namespace into multiple meshes in the future. Use an array of selectors instead of a single one on mesh resource. Selectors among selectors array are \u201c OR \u201d relationship. And within a selector it\u2019s AND relationship(e.g. resources must match both namespaceSelector & resourceSelector to be member of the mesh) apiVersion : appmesh.k8s.aws/v1beta2 kind : Mesh metadata : name : my-mesh spec : selectors : - namespaceSelector : matchLabels : mesh : my-mesh - namespaceSelector : matchLabels : my-mesh : \"true\" resourceSelector : matchLabels : my-mesh : \"true\" Pros: * let user freely define selectors for different use cases. Cons: * over complicated for simple use cases. users should be properly organize their labels to achieve similar effect even under the single selector proposal.","title":"Use selector on Mesh to denote mesh membership for resources within namespaces"},{"location":"reference/api_design/#use-selector-on-virtualnode-to-denote-node-membership","text":"We use a podSelector to match pods that should be member of virtualNode. Note, we don\u2019t use a namespace selector here since we want to enforce pods are in same namespace of virtualNode resource. The label selectors of two virtualNode within namespace must not overlap. Controller should error if detected conflicting virtualNode for given pod. The injector will be changed to work using following logic: 1. for each pod created, it match against all virtualNode\u2019s selector within namespace. 2. If it finds a single match, injects according to virtualNode\u2019s mesh and name. 3. If it finds multiple match, it reports error. 4. The global \" APPMESH_NAME \" environment variable on injector deployment and annotation \" appmesh.k8s.aws/mesh \" and \" appmesh.k8s.aws/virtualNode \" annotation on pod will no longer be supported. apiVersion : appmesh.k8s.aws/v1beta1 kind : VirtualNode metadata : namespace : my-app-ns name : my-app-node spec : ...other fields... podSelector : matchLabels : app : my-app-node --- apiVersion : apps/v1 kind : Deployment metadata : namespace : my-app-ns name : my-app-node-deployment spec : selector : matchLabels : app : my-app-node replicas : 5 template : metadata : labels : app : my-app-node spec : containers : - name : app image : awesomeimage:v1.0.0 ports : - containerPort : 80 Alternative designs considered: The injector use an environment variable for default mesh name and allow override per pod with annotation appmesh.k8s.aws/mesh . Also, the virtualNodeName is derived from pod\u2019s replicaSet\u2019s deployment name and allows override per pod with annotation appmesh.k8s.aws/virtualNode ( this is existing design used in our controller ) Pros: * easy to implement since all information is on pod itself. Cons: * The default behavior to resolve mesh relies on the fact that there is only one mesh in cluster, which conflicts with our multiple mesh per cluster support. * The default behavior to resolve virtualNode is not k8s native. it implies pods are managed by replicaSets then by deployments, which is tricky and not always true. * the annotations are intrusive to user\u2019s application.","title":"Use selector on VirtualNode to denote node membership"},{"location":"tutorials/walkthroughs/","text":"AppMesh Walkthroughs \u00b6 howto-k8s-alb : This example shows how to use ALB Ingress Controller with targets registered as virtual-nodes under App Mesh. howto-k8s-cloudmap : This example shows how Kubernetes deployments can use AWS CloudMap for service-discovery when using App Mesh. howto-k8s-fargate : This example shows how to use AppMesh with EKS on Fargate. howto-k8s-grpc :This example shows how to manage gRPC routes in App Mesh using Kubernetes deployments. howto-k8s-http-headers : This example shows how http routes can use headers for matching incoming requests. howto-k8s-http2 : This example shows how to manage HTTP/2 routes in App Mesh using Kubernetes deployments howto-k8s-retry-policy : This example shows how retry-policies can be used to for Kubernetes applications within the context of App Mesh.","title":"Walkthroughs"},{"location":"tutorials/walkthroughs/#appmesh-walkthroughs","text":"howto-k8s-alb : This example shows how to use ALB Ingress Controller with targets registered as virtual-nodes under App Mesh. howto-k8s-cloudmap : This example shows how Kubernetes deployments can use AWS CloudMap for service-discovery when using App Mesh. howto-k8s-fargate : This example shows how to use AppMesh with EKS on Fargate. howto-k8s-grpc :This example shows how to manage gRPC routes in App Mesh using Kubernetes deployments. howto-k8s-http-headers : This example shows how http routes can use headers for matching incoming requests. howto-k8s-http2 : This example shows how to manage HTTP/2 routes in App Mesh using Kubernetes deployments howto-k8s-retry-policy : This example shows how retry-policies can be used to for Kubernetes applications within the context of App Mesh.","title":"AppMesh Walkthroughs"}]}